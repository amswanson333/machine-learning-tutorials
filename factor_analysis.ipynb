{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags: \n",
    "```\n",
    "dimensionality reduction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the data collected for analysis may have underlying, latent, or hidden factors that contribute to the patterns in the data. The factors or features of the dataset may be able to be combined in such a way that they represent a different, latent factor that was not initially considered.\n",
    "\n",
    "Consider the following features for a fitness dataset:\n",
    "\n",
    "| Feature |\n",
    "| --- |\n",
    "| Total Number of Body-Weight Squats |\n",
    "| Total Number of Push-Ups |\n",
    "| Total Number of Sit-Ups |\n",
    "| Total Number of Pull-Ups |\n",
    "| Minutes per Mile Running |\n",
    "| Total Number of Lunges |\n",
    "| Max Bench Press Weight |\n",
    "| Max Squat Weight |\n",
    "\n",
    "It may be possible to reduce the number of features to the latent factors that contribute to those measurements. Using the above features as an example, it's possible that features such as total number of body-weight squats, total number of lunges, and max squat weight can be represented by a Latent Factor of lower body fitness. Push-ups, max bench press, and pull-ups may likewise be better represented by a Latent Factor of upper body fitness. Sometimes these Latent Factors cannot be directly measured, but the features used to indirectly measure them are present within the features. In some ways it is like determining what a root cause for multiple features might be.\n",
    "\n",
    "The benefits of Factor Analysis is primarily in investigating the root causes of the features as well as when attempting to find explainable ways to reduce the dimensionality of the data primarily for data visualization purposes. It would be impossible to visualize all of the above features simultaneously, but having two axes for upper body fitness and lower body fitness may be a useful way to effectively summarize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of Factor Analysis is very similar to how Principal Components Analysis is performed. In both approaches the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preamble: load python libraries and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and labels from the breast cancer dataset\n",
    "data = load_breast_cancer().data\n",
    "labels = load_breast_cancer().feature_names\n",
    "\n",
    "# Create a DataFrame with the data and labels\n",
    "df = pd.DataFrame(data, columns=labels)\n",
    "\n",
    "# Dixplay the first 5 rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally the best practice to standardize the data you are working with before performing additional transformations. This keeps the data from each feature in the same scale, preventing certain features from dominating the later transformations.\n",
    "\n",
    "The scaled term $Z$ in this case is equal to the difference between it's original value $X$, the average value over the feature $\\bar{X}$, divided by the standard deviation of the feature $\\hat{\\sigma}$:\n",
    "$$\n",
    "Z = \\frac{X - \\bar{X}}{\\hat{\\sigma}}\n",
    "$$\n",
    "\n",
    "There are other types of scaling, but this is most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>network-node-hemi</th>\n",
       "      <th>1-1-lh</th>\n",
       "      <th>1.1-1-rh</th>\n",
       "      <th>2-1-lh</th>\n",
       "      <th>2.1-1-rh</th>\n",
       "      <th>3-1-lh</th>\n",
       "      <th>3.1-1-rh</th>\n",
       "      <th>4-1-lh</th>\n",
       "      <th>4.1-1-rh</th>\n",
       "      <th>5-1-lh</th>\n",
       "      <th>5.1-1-rh</th>\n",
       "      <th>...</th>\n",
       "      <th>16.5-3-rh</th>\n",
       "      <th>16.6-4-lh</th>\n",
       "      <th>16.7-4-rh</th>\n",
       "      <th>17-1-lh</th>\n",
       "      <th>17.1-1-rh</th>\n",
       "      <th>17.2-2-lh</th>\n",
       "      <th>17.3-2-rh</th>\n",
       "      <th>17.4-3-lh</th>\n",
       "      <th>17.5-3-rh</th>\n",
       "      <th>17.6-4-lh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.146477</td>\n",
       "      <td>1.724226</td>\n",
       "      <td>0.086023</td>\n",
       "      <td>0.909846</td>\n",
       "      <td>1.007335</td>\n",
       "      <td>-1.508693</td>\n",
       "      <td>1.681734</td>\n",
       "      <td>0.934843</td>\n",
       "      <td>-1.033637</td>\n",
       "      <td>-0.048148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>-1.207393</td>\n",
       "      <td>1.835038</td>\n",
       "      <td>-0.711909</td>\n",
       "      <td>0.030446</td>\n",
       "      <td>0.207620</td>\n",
       "      <td>1.990501</td>\n",
       "      <td>-0.281671</td>\n",
       "      <td>2.923483</td>\n",
       "      <td>-0.809335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.136079</td>\n",
       "      <td>0.818788</td>\n",
       "      <td>-1.664163</td>\n",
       "      <td>-0.329550</td>\n",
       "      <td>-1.087056</td>\n",
       "      <td>-1.185072</td>\n",
       "      <td>-0.043070</td>\n",
       "      <td>-0.665321</td>\n",
       "      <td>0.563522</td>\n",
       "      <td>0.408692</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085169</td>\n",
       "      <td>-1.312511</td>\n",
       "      <td>3.018299</td>\n",
       "      <td>-0.426896</td>\n",
       "      <td>1.376380</td>\n",
       "      <td>-0.421138</td>\n",
       "      <td>0.894989</td>\n",
       "      <td>-1.061768</td>\n",
       "      <td>0.600043</td>\n",
       "      <td>-0.749945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.247527</td>\n",
       "      <td>1.188686</td>\n",
       "      <td>-1.298631</td>\n",
       "      <td>-0.319820</td>\n",
       "      <td>-0.724450</td>\n",
       "      <td>-0.037035</td>\n",
       "      <td>-2.324012</td>\n",
       "      <td>-3.000008</td>\n",
       "      <td>0.554291</td>\n",
       "      <td>0.955766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533812</td>\n",
       "      <td>0.154672</td>\n",
       "      <td>1.079189</td>\n",
       "      <td>0.853203</td>\n",
       "      <td>1.015228</td>\n",
       "      <td>0.034952</td>\n",
       "      <td>-0.653651</td>\n",
       "      <td>0.348756</td>\n",
       "      <td>-1.822061</td>\n",
       "      <td>0.130185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.378869</td>\n",
       "      <td>0.237533</td>\n",
       "      <td>-0.878612</td>\n",
       "      <td>-0.769683</td>\n",
       "      <td>-0.285322</td>\n",
       "      <td>0.519275</td>\n",
       "      <td>-1.481487</td>\n",
       "      <td>-2.049997</td>\n",
       "      <td>0.947325</td>\n",
       "      <td>0.287926</td>\n",
       "      <td>...</td>\n",
       "      <td>1.348673</td>\n",
       "      <td>1.129490</td>\n",
       "      <td>-0.253362</td>\n",
       "      <td>1.812035</td>\n",
       "      <td>0.126205</td>\n",
       "      <td>-0.065093</td>\n",
       "      <td>0.348671</td>\n",
       "      <td>-0.423770</td>\n",
       "      <td>-0.907845</td>\n",
       "      <td>-0.095631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.051387</td>\n",
       "      <td>-1.181505</td>\n",
       "      <td>-0.351113</td>\n",
       "      <td>-0.373429</td>\n",
       "      <td>-1.738375</td>\n",
       "      <td>0.105522</td>\n",
       "      <td>-2.219029</td>\n",
       "      <td>-1.716614</td>\n",
       "      <td>0.529796</td>\n",
       "      <td>0.083014</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805173</td>\n",
       "      <td>0.873977</td>\n",
       "      <td>-0.549790</td>\n",
       "      <td>1.411324</td>\n",
       "      <td>1.549633</td>\n",
       "      <td>0.753508</td>\n",
       "      <td>-0.339738</td>\n",
       "      <td>0.146762</td>\n",
       "      <td>0.124460</td>\n",
       "      <td>1.787865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "network-node-hemi    1-1-lh  1.1-1-rh    2-1-lh  2.1-1-rh    3-1-lh  3.1-1-rh  \\\n",
       "0                  1.146477  1.724226  0.086023  0.909846  1.007335 -1.508693   \n",
       "1                  1.136079  0.818788 -1.664163 -0.329550 -1.087056 -1.185072   \n",
       "2                  1.247527  1.188686 -1.298631 -0.319820 -0.724450 -0.037035   \n",
       "3                  0.378869  0.237533 -0.878612 -0.769683 -0.285322  0.519275   \n",
       "4                 -0.051387 -1.181505 -0.351113 -0.373429 -1.738375  0.105522   \n",
       "\n",
       "network-node-hemi    4-1-lh  4.1-1-rh    5-1-lh  5.1-1-rh  ...  16.5-3-rh  \\\n",
       "0                  1.681734  0.934843 -1.033637 -0.048148  ...   0.010200   \n",
       "1                 -0.043070 -0.665321  0.563522  0.408692  ...   1.085169   \n",
       "2                 -2.324012 -3.000008  0.554291  0.955766  ...   0.533812   \n",
       "3                 -1.481487 -2.049997  0.947325  0.287926  ...   1.348673   \n",
       "4                 -2.219029 -1.716614  0.529796  0.083014  ...   1.805173   \n",
       "\n",
       "network-node-hemi  16.6-4-lh  16.7-4-rh   17-1-lh  17.1-1-rh  17.2-2-lh  \\\n",
       "0                  -1.207393   1.835038 -0.711909   0.030446   0.207620   \n",
       "1                  -1.312511   3.018299 -0.426896   1.376380  -0.421138   \n",
       "2                   0.154672   1.079189  0.853203   1.015228   0.034952   \n",
       "3                   1.129490  -0.253362  1.812035   0.126205  -0.065093   \n",
       "4                   0.873977  -0.549790  1.411324   1.549633   0.753508   \n",
       "\n",
       "network-node-hemi  17.3-2-rh  17.4-3-lh  17.5-3-rh  17.6-4-lh  \n",
       "0                   1.990501  -0.281671   2.923483  -0.809335  \n",
       "1                   0.894989  -1.061768   0.600043  -0.749945  \n",
       "2                  -0.653651   0.348756  -1.822061   0.130185  \n",
       "3                   0.348671  -0.423770  -0.907845  -0.095631  \n",
       "4                  -0.339738   0.146762   0.124460   1.787865  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the DataFrame by subtracting the mean and dividing by the standard deviation\n",
    "for col in df.columns:\n",
    "    df[col] = ( df[col] - df[col].mean() ) / df[col].std()\n",
    "\n",
    "# Display the first 5 rows of the standardized DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other, built-in functions to perform scaling, the above is shown to illustrate what is actually happening. A typical workflow will use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler to the DataFrame and transform it\n",
    "df_scaled = scaler.fit_transform(df)    # Output is a numpy array\n",
    "\n",
    "# Convert the numpy array back to a DataFrame with the original labels\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=labels)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "**Factor Loadings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each factor $X_n$ is a linear combination of $m$ Latent Factors with an error term $\\epsilon_n$ such that:\n",
    "$$\n",
    "X_n = \\lambda_{n,1} f_1 + \\dots + \\lambda_{n,m} f_m + \\epsilon_n\n",
    "$$\n",
    "\n",
    "Or in matrix notation:\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{\\Lambda} \\mathbf{f} + \\mathbf{\\epsilon}\n",
    "$$\n",
    "\n",
    "The $\\lambda_{n,m}$ coefficients are referred to as the *factor loadings*, and the error terms $\\epsilon_n$ are called the *unique factors*.\n",
    "\n",
    "Each of the factors $f_m$ are common to each of the features. By determining the factor loading values how important each feature is for each factor, or how much the factor reflects the original features.\n",
    "\n",
    "Consider if we choose to use a number of Latent Factors equal to the initial number of features ($n = m$) then if we attempt to minimize the error terms then the factor loadings will be $1$ for one of the Latent Factors and $0$ for all others yielding $X_n = f_m$. Since one of the goals is to be able to visualize the data it is advisable to start with 2 or 3 Latent Factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $\\mathbf{\\Lambda}$ is equal to the covariance matrix of the data $\\mathbf{X}$ and the Latent Factors $\\mathbf{f}$.\n",
    "$$\n",
    "\\mathbf{\\Lambda} = \n",
    "\\begin{bmatrix}\n",
    "\\lambda_{1,1} & \\lambda_{1,2} & \\cdots & \\lambda_{1,m} \\\\\n",
    "\\lambda_{2,1} & \\lambda_{2,2} & \\cdots & \\lambda_{2,m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\lambda_{n,1} & \\lambda_{n,2} & \\cdots & \\lambda_{n,m} \n",
    "\\end{bmatrix}\n",
    "= \\text{cov}(\\mathbf{X}, \\mathbf{f})\n",
    "$$\n",
    "\n",
    "Using the covariance operator on the previous notation gives:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{cov}(\\mathbf{X}) = \\mathbf{\\Sigma} = & \\text{cov}(\\mathbf{\\Lambda} \\mathbf{f} + \\mathbf{\\epsilon}) \\\\\n",
    "    = & \\text{cov}(\\mathbf{\\Lambda} \\mathbf{f}) + \\text{cov}(\\mathbf{\\epsilon}) \\\\\n",
    "    = & \\mathbf{\\Lambda I \\Lambda}^\\top + \\mathbf{\\Psi} \\\\\n",
    "    = & \\mathbf{\\Lambda \\Lambda}^\\top + \\mathbf{\\Psi}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the covariance matrix for the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 62)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrix = np.cov(df.T)\n",
    "cov_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: we use the transpose of the data so we get a covariance of the columns, not the rows.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the eigenvalue and eigenvectors for the covariance matrix.\n",
    "\n",
    "*Note: the covariance matrix is a symmetric matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalue, eigenvector = np.linalg.eig(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger eigenvalues are associated with the factor loadings (eigenvectors) that have the most significantly contributing factors. We therefore sort by the magnitutde of the eigenvalues in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort descending by eigenvalue\n",
    "\n",
    "idx = np.argsort(-eigenvalue)\n",
    "fa_eigenvalue = eigenvalue[idx]\n",
    "fa_eigenvector = eigenvector[:,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step can be checked by evaluating the eigen-decomposition such that:\n",
    "$$\n",
    "\\Sigma = \\bf{V} \\bf{E} \\bf{V}^{T}\n",
    "$$\n",
    "Where $\\bf{V}$ is the matrix with the eigenvectors as the columns, $\\bf{E}$ is the matrix with the eigenvalues associated with each column on the diagonal, and $\\Sigma$ is the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the E matrix\n",
    "\n",
    "e_matrix = np.diag(fa_eigenvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the matrix multiplication\n",
    "\n",
    "sigma_matrix = fa_eigenvector @ e_matrix @ fa_eigenvector.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to the covariance matrix\n",
    "# the np.allclose method is used due to floating point precision errors\n",
    "\n",
    "np.allclose(sigma_matrix, cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the number of factors to use.\n",
    "\n",
    "A number of methods exist to determine how many factors to use, a handful follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaiser's Criteria** uses all eigenvalues (and corresponding eigenvectors) that are $\\ge 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaiser Criteria eigenvalues are: [10.399991640203382, 7.743590501606728, 4.325089934427531, 3.8531222876608306, 3.1320324428345905, 2.676425667354694, 2.340985108788398, 2.040920038881663, 1.6382493065525199, 1.6097022483529324, 1.3531497913378536, 1.3168355749163363, 1.227574437507627, 1.0557234760368766]\n"
     ]
    }
   ],
   "source": [
    "kaiser_values = [i for i in fa_eigenvalue if i >= 1]\n",
    "\n",
    "print(\"Kaiser Criteria eigenvalues are:\", kaiser_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jolliffe's Criteria** uses all eigenvalues (and corresponding eigenvectors) that are $\\ge 0.7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jolliffe's Criteria eigenvalues are: [10.399991640203382, 7.743590501606728, 4.325089934427531, 3.8531222876608306, 3.1320324428345905, 2.676425667354694, 2.340985108788398, 2.040920038881663, 1.6382493065525199, 1.6097022483529324, 1.3531497913378536, 1.3168355749163363, 1.227574437507627, 1.0557234760368766]\n"
     ]
    }
   ],
   "source": [
    "jolliffe_values = [i for i in fa_eigenvalue if i >= 1]\n",
    "\n",
    "print(\"Jolliffe's Criteria eigenvalues are:\", jolliffe_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scree Plot** plots all of the eigenvalues to visually determine where a \"bend\" or \"elbow\" occurs before it flattens out. This is used to gauge when eigenvalues stop having a meaningful contribution to the number of factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cumulative percentage series\n",
    "\n",
    "cumsum_percent = ( fa_eigenvalue.cumsum() / fa_eigenvalue.sum() ) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x-axis labels for the chart\n",
    "\n",
    "scree_x = [\"F\"+str(i) for i in range(1,len(fa_eigenvalue)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Data source must be a DataFrame or Mapping, not <class 'tuple'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFactor\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEigenvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39m(scree_x[:\u001b[38;5;241m50\u001b[39m], fa_eigenvalue[:\u001b[38;5;241m50\u001b[39m]))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:2341\u001b[0m, in \u001b[0;36mbarplot\u001b[1;34m(data, x, y, hue, order, hue_order, estimator, errorbar, n_boot, seed, units, weights, orient, color, palette, saturation, fill, hue_norm, width, dodge, gap, log_scale, native_scale, formatter, legend, capsize, err_kws, ci, errcolor, errwidth, ax, **kwargs)\u001b[0m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlen\u001b[39m:\n\u001b[0;32m   2339\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2341\u001b[0m p \u001b[38;5;241m=\u001b[39m _CategoricalAggPlotter(\n\u001b[0;32m   2342\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m   2343\u001b[0m     variables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, hue\u001b[38;5;241m=\u001b[39mhue, units\u001b[38;5;241m=\u001b[39munits, weight\u001b[38;5;241m=\u001b[39mweights),\n\u001b[0;32m   2344\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   2345\u001b[0m     orient\u001b[38;5;241m=\u001b[39morient,\n\u001b[0;32m   2346\u001b[0m     color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m   2347\u001b[0m     legend\u001b[38;5;241m=\u001b[39mlegend,\n\u001b[0;32m   2348\u001b[0m )\n\u001b[0;32m   2350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2351\u001b[0m     ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:67\u001b[0m, in \u001b[0;36m_CategoricalPlotter.__init__\u001b[1;34m(self, data, variables, order, orient, require_numeric, color, legend)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     58\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     65\u001b[0m ):\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data\u001b[38;5;241m=\u001b[39mdata, variables\u001b[38;5;241m=\u001b[39mvariables)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwide\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_variables(data, variables)\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m PlotData(data, variables)\n\u001b[0;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[0;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\_core\\data.py:57\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     53\u001b[0m     data: DataSource,\n\u001b[0;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[0;32m     55\u001b[0m ):\n\u001b[1;32m---> 57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[0;32m     58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_variables(data, variables)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\_core\\data.py:278\u001b[0m, in \u001b[0;36mhandle_data_source\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m    277\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData source must be a DataFrame or Mapping, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(err)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mTypeError\u001b[0m: Data source must be a DataFrame or Mapping, not <class 'tuple'>."
     ]
    }
   ],
   "source": [
    "sns.barplot(x=\"Factor\", y=\"Eigenvalue\", data=(scree_x[:50], fa_eigenvalue[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.array([[1,0.2,0.8],[0.2,1,0.3],[0.8,0.3,1]])\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalue, eigenvector = np.linalg.eig(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eigenvalue)\n",
    "print(eigenvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(-eigenvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_eigenvalue = eigenvalue[idx]\n",
    "fa_eigenvector = eigenvector[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_matrix = np.diag(fa_eigenvalue)\n",
    "print(e_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_matrix = np.array(fa_eigenvector)\n",
    "print(v_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_matrix @ e_matrix @ v_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_matrix_red = e_matrix[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_matrix_red = v_matrix[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = fa_eigenvector[:,0:2] * np.sqrt(fa_eigenvalue[0:2])\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_var = loadings @ loadings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness = np.diag(1 - common_var.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Covariance Matrix (Sigma):\\n\", sigma, \"\\n\")\n",
    "print(\"Eigenvalue Matrix (E):\\n\", e_matrix, \"\\n\")\n",
    "print(\"Eigenvector Matrix (V):\\n\", v_matrix, \"\\n\")\n",
    "print(\"Factor Loadings (Lambda):\\n\", loadings, \"\\n\")\n",
    "print(\"Common Variance Matrix (Lambda * Lambda.T):\\n\", common_var, \"\\n\")\n",
    "print(\"Uniqueness Matrix (psi):\\n\", uniqueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_est = common_var + uniqueness\n",
    "print(sigma_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Communalities are:\", common_var.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1_variance = np.sum(np.square(loadings[:,0]))\n",
    "feature2_variance = np.sum(np.square(loadings[:,1]))\n",
    "\n",
    "print(feature1_variance, feature2_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
